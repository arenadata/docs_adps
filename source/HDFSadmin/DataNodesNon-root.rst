DataNodes от Non-root
=======================

В главе описываются правила запуска *DataNodes* от пользователя без прав *root*.

Исторически сложилось так, что часть конфигурации безопасности для **HDFS** задействовала запуск *DataNode* от пользователя *root* и привязала привилегированные порты для конечных точек сервера. Это было сделано для решения проблемы безопасности, то есть если задание **MapReduce** запущено, а *DataNode* остановился, задачу **MapReduce** можно привязать к порту *DataNode* и потенциально сделать что-то вредоносное. Решением подобных случаев стал запуск *DataNode* от пользователя *root* и использование привилегированных портов. При этом только пользователь *root* может получить доступ к привилегированным портам.

Для безопасного запуска *DataNodes* от пользователя без прав *root* можно использовать **Simple Authentication and Security Layer** (**SASL**), который применяется для обеспечения безопасной связи на уровне протокола.

.. important:: Важно выполнить переход от использования *root* к запуску DataNodes с SASL в конкретной последовательности по всему кластеру. В противном случае может возникнуть риск простоя приложения

Для переноса существующего кластера, использующего аутентификацию *root*, сначала необходимо убедиться, что версия **2.6.0** (или более поздняя) развернута для всех узлов кластера, а также для любых внешних приложений, которые необходимо подключить к кластеру. Только версии **2.6.0 +** из HDFS-клиента могут подключаться к *DataNode*, использующему **SASL** для аутентификации протокола передачи данных, поэтому важно, чтобы все абоненты имели необходимую версию перед переходом. 

После развертывания версии **2.6.0** (или более поздней) необходимо обновить конфигурацию любых внешних приложений, чтобы включить **SASL**. Если для **SASL** включен клиент **HDFS**, он может успешно подключиться к *DataNode*, работающему с аутентификацией *root* или аутентификацией **SASL**. Изменение конфигурации для всех клиентов гарантирует, что последующие изменения конфигурации в *DataNodes* не нарушат работу приложений. Наконец, каждый отдельный *DataNode* может быть перенесен путем изменения его конфигурации и перезапуска. Допустимо временно сочетать некоторые *DataNodes* с аутентификацией *root* и некоторые *DataNodes*, работающие с аутентификацией **SASL**, в течение периода миграции, поскольку клиент **HDFS**, подключенный для **SASL**, может подключаться к обоим.


Настройка DataNode SASL
------------------------

Для настройки **DataNode SASL** с безопасным запуском *DataNode* от *non-root* пользователя необходимо выполнить действия:

**1. Выключить DataNode**

**2. Включить SASL**

Чтобы включить **DataNode SASL**, необходимо в файле */etc/hadoop/conf/hdfs-site.xml* настроить свойство *dfs.data.transfer.protection*, задав одно из следующих значений:

+ ``authentication`` -- устанавливает взаимную аутентификацию между клиентом и сервером;
+ ``integrity`` -- в дополнение к аутентификации гарантирует, что man-in-the-middle не может вмешиваться в сообщения, обмен которыми осуществляется между клиентом и сервером;
+ ``privacy`` -- в дополнение к функциям ``authentication`` и ``integrity`` он также полностью шифрует сообщения, обмен которыми осуществляется между клиентом и сервером.

Также необходимо установить для свойства *dfs.http.policy* значение ``HTTPS_ONLY``. При этом следует указать порты для **DataNode RPC** и HTTP-серверов.

Например:
::

 <property>
    <name>dfs.data.transfer.protection</name>
    <value>integrity</value>
  </property>
 
  <property>
    <name>dfs.datanode.address</name>
    <value>0.0.0.0:10019</value>
  </property>
 
  <property>
    <name>dfs.datanode.http.address</name>
    <value>0.0.0.0:10022</value>
  </property>
 
  <property>
    <name>dfs.http.policy</name>
    <value>HTTPS_ONLY</value>
  </property>

.. important:: Параметр шифрования *dfs.encrypt.data.transfer=true* похож на *dfs.data.transfer.protection=privacy*. Эти два параметра являются взаимоисключающими, поэтому они не должны устанавливаться вместе. В случае если оба параметра установлены, *dfs.encrypt.data.transfer* не используется

**3. Обновить настройки экосистемы**

В файле */etc/hadoop/conf/hadoop-env.xml* изменить параметр:
::

 #On secure datanodes, user to run the datanode as after dropping privileges export HADOOP_SECURE_DN_USER=

Строка экспорта *HADOOP_SECURE_DN_USER=hdfs* включает устаревшую конфигурацию безопасности и, чтобы включить **SASL**, должна быть установлена на пустое значение.

**4. Запустить DataNode**

