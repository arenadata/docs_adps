Hadoop: Capacity Scheduler
============================

В главе описывается **CapacityScheduler** -- подключаемый планировщик для **Hadoop**, позволяющий при мультитенантности безопасно совместно использовать большой кластер таким образом, чтобы для приложений своевременно распределялись ресурсы в условиях ограниченно выделенных мощностей.

**CapacityScheduler** предназначен для запуска приложений **Hadoop** в виде общего мультитенантного кластера удобным для оператора способом при максимальной пропускной способности и загрузке кластера.

Традиционно каждая организация имеет свой собственный набор вычислительных ресурсов, которые имеют достаточную производительность для соответствия SLA предприятия в пиковых или около пиковых условиях. Как правило, это приводит к низкой средней загрузке и накладным расходам на управление несколькими независимыми кластерами по одному на каждую организацию. Поэтому совместное использование кластеров между несколькими организациями -- это рентабельный способ запуска крупных Hadoop-инсталляций, так как это позволяет пользоваться преимуществами  масштаба, не создавая частных кластеров. Однако организации обеспокоены совместным использованием кластера в вопросе использования другими предприятиями ресурсов, критически важных для их собственного SLA.

**CapacityScheduler** предназначен для совместного использования большого кластера, предоставляя при этом каждой организации гарантии производительности. Основная идея заключается в том, что доступные ресурсы в кластере **Hadoop** распределяются между несколькими предприятиями. Дополнительным преимуществом является то, что организация может получить доступ к любой избыточной мощности, не используемой другими. Это обеспечивает гибкость экономически эффективным образом.

Совместное использование кластеров требует строгой мультитенантности, поскольку каждому предприятию должна быть обеспечена производительность и безопасность, чтобы гарантировать, что общий кластер защищен от любого постороннего приложения или пользователя. **CapacityScheduler** предоставляет обязательный набор ограничений, гарантирующих, что отдельное приложение, пользователь или очередь не могут использовать непропорционально большое количество ресурсов в кластере. Кроме того, для обеспечения справедливости и стабильности кластера планировщик предоставляет для инициализированных и ожидающих приложений от одного пользователя очереди и ограничения.

Основной абстракцией, предоставляемой **CapacityScheduler**, является концепция очередей. Они обычно настраиваются администраторами и отражают экономику общего кластера.

С целью дополнительного контроля и предсказуемости при совместном использовании ресурсов **CapacityScheduler** также поддерживает иерархические очереди, чтобы обеспечить распределение ресурсов между под-очередями среди приложений внутри одной организации, прежде чем другим очередям будет позволено использовать свободные ресурсы.


Функции
-----------

**CapacityScheduler** поддерживает следующие функции:

+ Иерархические очереди (Hierarchical Queues). 

Поддерживается иерархия очередей, обеспечивающая совместное использование ресурсов между под-очередями внутри организации, прежде чем другим очередям будет позволено использовать свободные ресурсы, что обеспечивает больший контроль и предсказуемость.

+ Гарантии поизводительности (Capacity Guarantees). 

Очереди распределяются по части пропускной способности сети в том смысле, что в их распоряжении находится определенная производительность ресурсов. Все приложения, отправленные в очередь, имеют доступ к производительности, выделенной для этой конкретной очереди. Для каждой очереди ограничения пропускной способности настраиваются администраторами и могут быть как мягкими, так и жесткими.

+ Безопасность (Security). 

Каждая очередь имеет строгие списки ACL, которые контролируют, какие пользователи могут отправлять приложения в отдельные очереди. Кроме того, существуют средства защиты, гарантирующие, что пользователи не смогут просматривать и/или изменять приложения других пользователей. Также поддерживаются роли для каждой очереди и системного администратора.

+ Эластичность (Elasticity). 

Свободные ресурсы могут быть распределены на любые очереди. Когда в будущем от очередей, работающих с пониженной производительностью, возникает потребность в ресурсах, то по мере выполнения запланированных на этих ресурсах задач они назначаются требуемым приложениям (также поддерживается преимущественное право -- preemption). Это гарантирует, что ресурсы доступны для очередей предсказуемо и гибко, тем самым предотвращая искусственное разделение ресурсов в кластере и помогая их использованию.

+ Мультитенантность (Multi-tenancy). 

Предоставляется набор ограничений для предотвращения монополизации ресурсов очереди или кластера одним приложением, пользователем или очередью, чтобы гарантировать, что кластер не перегружен.

+ Работоспособность (Operability):

  + Конфигурация во время выполнения (Runtime Configuration) -- определения и свойства очереди, такие как производительность и списки ACL, могут быть изменены во время выполнения безопасным способом администраторами, минимизируя неудобства для пользователей. Кроме того для пользователей и администраторов предусмотрена консоль, позволяющая просматривать текущее распределение ресурсов по различным очередям в системе. Администраторы могут добавлять дополнительные очереди во время выполнения, но очереди не могут быть удалены во время выполнения, если она не остановлена и имеет ожидающие/запущенные приложения.

  + Дренаж приложений (Drain applications) -- администраторы могут останавливать очереди во время выполнения, чтобы гарантировать, что пока существующие приложения не будут завершены, новые не смогут быть направлены. Если очередь находится в состоянии *STOPPED*, новые приложения не могут быть направлены ни ей самой, ни какой-либо из ее дочерних очередей. Текущие приложения продолжают выполняться и, таким образом, очередь может быть аккуратно дренажирована. Администраторы также могут запускать остановленные очереди.

+ Планирование на основе ресурсов (Resource-based Scheduling).

Поддержка ресурсоемких приложений, в которых приложение может опционально определять более высокие требования к ресурсам, чем по умолчанию, тем самым приспосабливая приложения с различными требованиями к ресурсам. В настоящее время память является поддерживаемым требованием к ресурсам.

+ Маппинг очереди на основе пользователя или группы (Queue Mapping based on User or Group).

Функция позволяет пользователям сопоставлять работу с определенной очередью на основе пользователя или группы.

+ Приоритетное планирование (Priority Scheduling). 

Функция позволяет направлять приложения и планировать их с разными приоритетами. Более высокое целочисленное значение указывает на более высокий приоритет для приложения. В настоящее время приоритет приложения поддерживается только для политики упорядочения *FIFO*.

+ Конфигурация абсолютных ресурсов (Absolute Resource Configuration).

Администраторы могут указывать абсолютные ресурсы для очереди вместо предоставления значений в процентах. Это обеспечивает лучший контроль для администраторов в целях настройки необходимого количества ресурсов для конкретной очереди.

+ Динамическое автоматическое создание и управление конечными очередями (Dynamic Auto-Creation and Management of Leaf Queues). 

Функция поддерживает автоматическое создание конечных очередей в сочетании с маппингом очередей, которое в настоящее время поддерживает сопоставления очередей на основе групп пользователей для размещения приложений в очереди. Планировщик также поддерживает управление производительностью для этих очередей на основе политики, настроенной в родительской очереди.


Конфигурация
----------------

Чтобы настроить **ResourceManager** для использования **CapacityScheduler**, необходимо установить в файле *conf/yarn-site.xml* свойство ``yarn.resourcemanager.scheduler.class`` со значением ``org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler``.

*etc/hadoop/capacity-scheduler.xml* -- файл конфигурации для **CapacityScheduler**.

**CapacityScheduler** имеет предопределенную очередь с именем *root*, все очереди в системе являются дочерними по отношению к ней. Очереди можно настроить в ``yarn.scheduler.capacity.root.queues`` со списком дочерних очередей, разделенных запятыми.

Конфигурация для **CapacityScheduler** для настройки иерархии очередей использует концепцию, называемую *путь к очереди* (*queue path*). Путь к очереди -- это полный путь иерархии очереди, начиная с *root*, со знаком точки ``.`` в качестве разделителя.

Дочерние элементы очереди могут быть определены с помощью настройки ``yarn.scheduler.capacity.<queue-path>.queues``. Дочерние очереди при этом не наследуют свойства напрямую от родителя, если не указано иное.

Пример с тремя дочерними очередями верхнего уровня *a*, *b* и *c* и некоторыми подпоследовательностями для *a* и *b*:

::

 <property>
   <name>yarn.scheduler.capacity.root.queues</name>
   <value>a,b,c</value>
   <description>The queues at the this level (root is the root queue).
   </description>
 </property>
 
 <property>
   <name>yarn.scheduler.capacity.root.a.queues</name>
   <value>a1,a2</value>
   <description>The queues at the this level (root is the root queue).
   </description>
 </property>
 
 <property>
   <name>yarn.scheduler.capacity.root.b.queues</name>
   <value>b1,b2,b3</value>
   <description>The queues at the this level (root is the root queue).
   </description>
 </property>


Свойства очереди
^^^^^^^^^^^^^^^^^^

Распределение ресурсов
~~~~~~~~~~~~~~~~~~~~~~~

``yarn.scheduler.capacity.<queue-path>.capacity`` -- пропускная способность очереди ИЛИ минимальная пропускная способность очереди абсолютных ресурсов, указывается в процентах в виде числа с плавающей запятой (float, например, *12.5*). Сумма производительности для всех очередей на каждом уровне должна быть равна *100*. Однако, если настроен абсолютный ресурс, сумма абсолютных ресурсов дочерних очередей может быть меньше абсолютной производительности родительского ресурса. Приложения в очереди могут потреблять больше ресурсов, чем пропускная способность очереди, если есть свободные ресурсы, обеспечивающие эластичность.

``yarn.scheduler.capacity.<queue-path>.maximum-capacity`` -- максимальная пропускная способность очереди ИЛИ максимальная пропускная способность очереди абсолютных ресурсов, указывается в процентах в виде числа с плавающей запятой (float). Параметр ограничивает эластичность для приложений в очереди: 1) Значение находится в диапазоне от *0* до *100*; 2) Администратор должен убедиться, что абсолютная максимальная производительность больше или равна абсолютной производительности для каждой очереди. Кроме того, установка значения в *-1* задает максимальную производительность в *100%*.

``yarn.scheduler.capacity.<queue-path>.minimum-user-limit-percent`` -- каждая очередь устанавливает ограничение на процент ресурсов, выделяемых пользователю в любой момент времени при потребности в ресурсах. Пользовательское ограничение может варьироваться между минимальным и максимальным значением. Минимум устанавливает данное свойство, а максимум зависит от количества отправивших приложение пользователей. Например, значение свойства равно *25*. Тогда если два пользователя отправляют приложения в очередь, ни один из них не может использовать более *50%* ресурсов очереди. Если третий пользователь отправляет приложение, то ни один пользователь не может использовать более *33%* ресурсов очереди. При наличии *4 или более* пользователей ни один из них не может использовать более *25%* ресурсов очереди. Значение *100* подразумевает, что ограничения для пользователей не вводятся. По умолчанию устанавливается значение *100*. Значение указывается как целое число (integer).

``yarn.scheduler.capacity.<queue-path>.user-limit-factor`` -- множество пропускной способности очереди, которое может быть настроено так, чтобы позволить пользователю получить больше ресурсов. По умолчанию значение равно *1*, что гарантирует, что пользователь никогда не сможет получить больше, чем настроенная производительность очереди, независимо от того, насколько простаивает кластер. Значение указывается как число с плавающей запятой (float).

``yarn.scheduler.capacity.<queue-path>.maximum-allocation-mb`` -- максимальный лимит памяти для каждой очереди, выделяемый каждому запросу контейнера в Resource Manager. Параметр переопределяет конфигурацию кластера ``yarn.scheduler.maximum-allocation-mb``. Значение должно быть меньше или равно максимуму кластера.

``yarn.scheduler.capacity.<queue-path>.maximum-allocation-vcores`` -- максимальный лимит виртуальных ядер для каждой очереди, выделяемый каждому запросу контейнера в Resource Manager. Параметр переопределяет конфигурацию кластера ``yarn.scheduler.maximum-allocation-vcores``. Значение должно быть меньше или равно максимуму кластера.

``yarn.scheduler.capacity.<queue-path>.user-settings.<user-name>.weight`` -- это значение с плавающей запятой (float), которое используется для вычисления предельных значений ресурсов пользователя среди пользователей в очереди. Значение определяет по весу каждого пользователя в большей или меньшей степени, относительно других пользователей в очереди. Например, если пользователь *A* должен получить на *50%* больше ресурсов в очереди, чем пользователи *B* и *C*, это свойство должно быть установлено равным *1,5* для пользователя *A*. При этом для пользователей *B* и *C* должно оставаться значение по умолчанию *1.0*.

Распределение ресурсов с помощью Absolute Resources
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**CapacityScheduler** поддерживает настройку абсолютных ресурсов вместо предоставления процентной пропускной способности очереди. Как упоминается в конфигурации для ``yarn.scheduler.capacity.<queue-path>.capacity`` и ``yarn.scheduler.capacity.<queue-path>.max-capacity``, администратор может указать значение абсолютного ресурса, например, ``[memory=10240,vcores=12]``. Это допустимая конфигурация, указывающая *10 ГБ* памяти и *12 VCores*.

Ограничения для запущенных и ожидающих приложений
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Для управления запущенными и ожидающими приложениями **CapacityScheduler** поддерживает следующие параметры:

``yarn.scheduler.capacity.maximum-applications / yarn.scheduler.capacity.<queue-path>.maximum-applications`` -- максимальное количество приложений в системе, которые могут быть одновременно активными (как запущенными, так и ожидающими). Ограничения в каждой очереди прямо пропорциональны пропускной способности очереди и пользовательским лимитам. Это жесткое ограничение, и любые поданные при его достижении приложения отклоняются. По умолчанию значение равно *10000*. Параметр может быть установлен для всех очередей с помощью ``yarn.scheduler.capacity.maximum-applications``, а также может быть переопределен для каждой очереди путем задания ``yarn.scheduler.capacity.<queue-path>.maximum-applications``. Параметр должен представлять собой целочисленное значение (integer).

``yarn.scheduler.capacity.maximum-am-resource-percent / yarn.scheduler.capacity.<queue-path>.maximum-am-resource-percent`` -- максимальный процент ресурсов в кластере, которые могут быть использованы для запуска мастера (application masters), контролирующего количество одновременно работающих приложений. Ограничения в каждой очереди прямо пропорциональны пропускной способности очереди и пользовательским лимитам. Указывается как число с плавающей запятой (float), то есть значение *0.5* равно *50%*. По умолчанию задается *10%*. Значение может быть установлено для всех очередей с помощью параметра ``yarn.scheduler.capacity.maximum-am-resource-percent``, а также может быть переопределено для каждой очереди путем задания ``yarn.scheduler.capacity.<queue-path>.maximum-am-resource-percent``.

Администрирование и разрешения очереди
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

``yarn.scheduler.capacity.<queue-path>.state`` -- статус очереди: *RUNNING* или *STOPPED*. Если очередь находится в состоянии *STOPPED*, новые приложения не могут быть отправлены ни ей самой, ни какой-либо из ее дочерних очередей. Таким образом, если очередь *root* остановлена, никакие приложения не могут быть переданы всему кластеру, текущие приложения продолжают выполняться, и очередь может быть аккуратно дренажирована. Значение указывается в виде именованной константы (enumeration).

``yarn.scheduler.capacity.root.<queue-path>.acl_submit_applications`` -- список ACL, который контролирует, кто может подавать приложения в конкретную очередь. Если у пользователя/группы есть необходимые списки управления доступом в очереди или в одной из ее родительских очередей в иерархии, то пользователь/группа может подаваться. Списки ACL для этого свойства наследуются из родительской очереди, если не указано иное.

``yarn.scheduler.capacity.root.<queue-path>.acl_administer_queue`` -- список ACL, который контролирует, кто может администрировать приложения в конкретной очереди. Если у пользователя/группы есть необходимые ACL в очереди или в одной из ее родительских очередей в иерархии, то пользователь/группа может администрировать приложения. Списки ACL для этого свойства наследуются из родительской очереди, если не указано иное.

ACL имеет форму *user1,user2 space group1,group2*. Особое значение ``*`` подразумевает *все*. Особое значение ``space`` подразумевает *никто*. Значение по умолчанию ``*`` для очереди *root*, если не указано иное.

Маппинг очереди на основе пользователя или группы
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

``yarn.scheduler.capacity.queue-mappings`` -- конфигурация определяет маппинг пользователя или группы в определенную очередь. Можно сопоставить одного пользователя или список пользователей с очередями. Синтаксис: ``[u or g]:[name]:[queue_name][,next_mapping]``. Обозначение *u* или *g* указывает, предназначено ли сопоставление для пользователя или группы соответственно; *name* указывает имя пользователя или имя группы. Чтобы указать пользователя, отправившего приложение, можно использовать *%user*. Обозначение *queue_name* указывает имя очереди, для которой должно маппироваться приложение. Чтобы указать имя очереди, совпадающее с именем пользователя, можно использовать *%user*. Чтобы указать имя очереди, совпадающее с именем основной группы, к которой принадлежит пользователь, можно использовать *%primary_group*.

``yarn.scheduler.capacity.queue-mappings-override.enable`` -- функция используется для задания возможности переопределения указанных пользователем очередей. Это логическое значение (boolean), и значением по умолчанию является *false*.

Пример:

::

  <property>
    <name>yarn.scheduler.capacity.queue-mappings</name>
    <value>u:user1:queue1,g:group1:queue2,u:%user:%user,u:user2:%primary_group</value>
    <description>
      Here, <user1> is mapped to <queue1>, <group1> is mapped to <queue2>, 
      maps users to queues with the same name as user, <user2> is mapped 
      to queue name same as <primary group> respectively. The mappings will be 
      evaluated from left to right, and the first valid mapping will be used.
    </description>
  </property>


Срок приложений в очереди
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

``yarn.scheduler.capacity.<queue-path>.maximum-application-lifetime`` -- максимальное время жизни отправленного в очередь приложения, задается в секундах. Любое меньшее или равное нулю значение считается как отключенное и является жестким лимитом времени для всех приложений в этой очереди. Если задано положительное значение параметра, любое приложение, отправленное в данную очередь, уничтожается после превышения настроенного срока. Пользователь также может указать срок для каждого приложения в контексте. Срок пользователя переопределяется, если он превышает максимальное время жизни очереди. Это конфигурация на определенный момент времени. Настройка слишком низкого значения приводит к быстрому уничтожению приложения. Функция применима только для leaf-очереди.

``yarn.scheduler.capacity.root.<queue-path>.default-application-lifetime`` -- время жизни отправленного в очередь приложения по умолчанию, задается в секундах. Любое меньшее или равное нулю значение считается как отключенное. Если пользователь отправляет приложение с незаданным значением срока, то оно задается автоматически. Это конфигурация на определенный момент времени. Примечание: время жизни по умолчанию не может превышать максимальное время жизни. Функция применима только для leaf-очереди.


Настройка приоритета приложения
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Приоритет приложения работает только совместно с политикой упорядочения по умолчанию *FIFO*. 

Приоритет по умолчанию для приложения может быть на уровне кластера и очереди:

+ Приоритет на уровне кластера -- у любого приложения, отправленного с приоритетом, превышающим приоритет *cluster-max*, происходит сброс приоритета до *cluster-max*. Файл конфигурации для приоритета *cluster-max* -- *$HADOOP_HOME/etc/hadoop/yarn-site.xml*. Параметр ``yarn.cluster.max-application-priority`` определяет максимальный приоритет приложения в кластере.

+ Приоритет на уровне leaf-очереди -- каждой leaf-очереди предоставляется приоритет администратора по умолчанию. Приоритет очереди по умолчанию используется для любого приложения, отправленного без заданного приоритета. Файл конфигурации для приоритета на уровне очереди -- *$HADOOP_HOME/etc/hadoop/capacity-scheduler.xml*. Параметр ``yarn.scheduler.capacity.root.<leaf-queue-path>.default-application-priority`` определяет приоритет приложения по умолчанию в leaf-очереди.

.. important:: Приоритет приложения не изменяется при перемещении приложения в другую очередь


Преимущественное право в Capacity Scheduler
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**CapacityScheduler** поддерживает возможность преимущественного права (preemption) контейнера от очередей, чье использование ресурсов превышает их гарантированную производительность. Для этого следующие параметры конфигурации должны быть включены в *yarn-site.xml*:

``yarn.resourcemanager.scheduler.monitor.enable`` -- включение набора периодического мониторинга (periodic monitors, указанных в ``yarn.resourcemanager.scheduler.monitor.policies``), влияющих на планировщик. Значением по умолчанию является *false*.

``yarn.resourcemanager.scheduler.monitor.policies`` -- список классов *SchedulingEditPolicy*, взаимодействующих с планировщиком. Настроенные политики должны быть совместимы с планировщиком. Значением по умолчанию является *org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy*, что совместимо с CapacityScheduler.

Следующие параметры конфигурации могут быть настроены в *yarn-site.xml* для управления преимущественным правом контейнеров, когда класс *ProportionalCapacityPreemptionPolicy* задан для *yarn.resourcemanager.scheduler.monitor.policies*:

``yarn.resourcemanager.monitor.capacity.preemption.observe_only`` -- если установлено значение *true*, следует запустить политику, но не влиять на кластер событиями preemption и kill. Значением по умолчанию является *false*.

``yarn.resourcemanager.monitor.capacity.preemption.monitoring_interval`` -- время между вызовами политики *ProportionalCapacityPreemptionPolicy* (в миллисекундах). Значение по умолчанию *3000*.

``yarn.resourcemanager.monitor.capacity.preemption.max_wait_before_kill`` -- время между запросом preemption из приложения и уничтожением контейнера (в миллисекундах). Значение по умолчанию *15000*.

``yarn.resourcemanager.monitor.capacity.preemption.total_preemption_per_round`` -- максимальный процент ресурсов для вытеснения по преимущественному праву за один раунд. Управляя этим значением, можно регулировать скорость, с которой контейнеры извлекаются из кластера. После вычисления общего желаемого преимущественного права политика сокращает его в пределах этого лимита. Значение по умолчанию *0.1*.

``yarn.resourcemanager.monitor.capacity.preemption.max_ignored_over_capacity`` -- максимальное количество ресурсов, превышающих по преимущественному праву заданную пропускную способность. Параметр определяет мертвую зону вокруг назначенной пропускной способности, которая помогает предотвратить колебания вокруг вычисленного заданного баланса. Высокие значения замедляют производительность и (при отсутствии *natural.completions*) могут препятствовать конвергенции к гарантированной производительности. Значение по умолчанию *0.1*.

``yarn.resourcemanager.monitor.capacity.preemption.natural_termination_factor`` -- учитывая вычисленное заданное преимущественное право, следует учесть контейнеры с истекающим сроком и выгрузить только этот процент дельты. Параметр определяет скорость геометрической конвергенции в мертвую зону (*MAX_IGNORED_OVER_CAPACITY*). Например, фактор высвобождения (termination factor) *0.5* восстанавливает почти *95%* ресурсов в пределах ``5 * #WAIT_TIME_BEFORE_KILL``, даже при отсутствии естественного завершения (natural termination). Значение по умолчанию составляет *0.2*.

**CapacityScheduler** поддерживает следующие конфигурации в *capacity-scheduler.xml* для управления преимущественным правом контейнеров приложений, отправляемых в очередь:

``yarn.scheduler.capacity.<queue-path>.disable_preemption`` -- конфигурацию можно установить в значение *true* для того, чтобы выборочно отключить преимущественное право контейнеров приложений, отправленных в указанную очередь. Свойство применяется только в том случае, если право preemption в масштабе всей системы включено путем настройки ``yarn.resourcemanager.scheduler.monitor.enable`` на *true* и ``yarn.resourcemanager.scheduler.monitor.policies`` на *ProportionalCapacityPreemptionPolicy*. Если данное свойство не установлено, то значение наследуется от родителя очереди. Значением по умолчанию является *false*.

``yarn.scheduler.capacity.<queue-path>.intra-queue-preemption.disable_preemption`` -- конфигурация может быть установлена в значение *true* для того, чтобы выборочно отключить внутри очереди преимущественное право контейнеров приложений, отправленных в указанную очередь. Свойство применяется только в том случае, если право preemption в масштабе всей системы включено путем настройки ``yarn.resourcemanager.scheduler.monitor.enable`` в значение *true*, ``yarn.resourcemanager.scheduler.monitor.policies`` на *ProportionalCapacityPreemptionPolicy* и ``yarn.resourcemanager.monitor.capacity.preemption.intra-queue-preemption.enabled`` в значение *true*. Если данное свойство не установлено, то значение наследуется от родителя очереди. Значением по умолчанию является *false*.


Свойства резервирования
~~~~~~~~~~~~~~~~~~~~~~~~

**CapacityScheduler** поддерживает параметры для управления созданием, удалением, обновлением и списком резервирований. Важно обратить внимание, что любой пользователь может обновлять, удалять или перечислять свои собственные резервирования. Если списки ACL-резервирования включены, но не определены, доступ будет иметь каждый. В приведенных далее примерах *<queue>* -- это имя очереди. Например, чтобы настроить ACL для управления резервированиями в очереди по умолчанию, следует использовать свойство ``yarn.scheduler.capacity.root.default.acl_administer_reservations``.

``yarn.scheduler.capacity.root.<queue>.acl_administer_reservations`` -- ACL, который контролирует, кто может управлять резервированием для указанной очереди. Если у данного пользователя/группы есть необходимые ACL в этой очереди, то он/она может отправлять, удалять, обновлять и составлять список всех резервирований. ACL для свойства не наследуются.

``yarn.scheduler.capacity.root.<queue>.acl_list_reservations`` -- ACL, который контролирует, кто может составлять список резервирований для указанной очереди. Если у данного пользователя/группы есть необходимые ACL в этой очереди, то он/она может составлять список всех приложений. ACL для свойства не наследуются.

``yarn.scheduler.capacity.root.<queue>.acl_submit_reservations`` -- ACL, который контролирует, кто может отправлять резервирования в указанную очередь. Если у данного пользователя/группы есть необходимые ACL в этой очереди, то он/она может отправлять резервирование. ACL для свойства не наследуются.


Настройка ReservationSystem с помощью CapacityScheduler
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**CapacityScheduler** поддерживает систему **ReservationSystem**, которая позволяет пользователям резервировать ресурсы заблаговременно. Таким образом приложение может запросить зарезервированные ресурсы во время выполнения, указав *reservationId*. Для этого могут быть настроены следующие параметры конфигурации в *yarn-site.xml*:

``yarn.resourcemanager.reservation-system.enable`` -- обязательный параметр: включить **ReservationSystem** в **ResourceManager**. Значение может быть только логическим (boolean), по умолчанию является *false*, то есть **ReservationSystem** не включена.

``yarn.resourcemanager.reservation-system.class`` -- необязательный параметр: имя класса **ReservationSystem**. Значение по умолчанию выбирается на основе настроенного планировщика, то есть если настроен **CapacityScheduler**, то классом является *CapacityReservationSystem*.

``yarn.resourcemanager.reservation-system.plan.follower`` -- необязательный параметр: имя класса *PlanFollower*, который запускается по таймеру и синхронизирует **CapacityScheduler** с *Plan* и наоборот. Значение по умолчанию выбирается на основе настроенного планировщика, то есть если настроен **CapacityScheduler**, то классом является *CapacitySchedulerPlanFollower*.

``yarn.resourcemanager.reservation-system.planfollower.time-step`` -- необязательный параметр: частота таймера *PlanFollower* (в миллисекундах). Значением по умолчанию является *1000*.

**ReservationSystem** интегрирована с иерархией очереди **CapacityScheduler** и может быть настроена для любой *LeafQueue*. Для этого в **CapacityScheduler** поддерживаются следующие параметры:

``yarn.scheduler.capacity.<queue-path>.reservable`` -- обязательный параметр: указывает **ReservationSystem**, что ресурсы очереди доступны для резервирования пользователями. Значение может быть только логическим (boolean), по умолчанию является *false*, то есть резервирование в *LeafQueue* не включено.

``yarn.scheduler.capacity.<queue-path>.reservation-agent`` -- необязательный параметр: имя класса для использования в целях определения реализации *ReservationAgent*, который принимает попытки разместить запрос пользователя на резервирование в *Plan*. Значением по умолчанию является *org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.AlignedPlannerWithGreedy*.

``yarn.scheduler.capacity.<queue-path>.reservation-move-on-expiry`` -- необязательный параметр, который указывает **ReservationSystem**, следует ли перемещать или уничтожать приложения в родительской резервируемой очереди (настроенной выше) по истечении срока действия соответствующего резервирования. Значение может быть только логическим (boolean), по умолчанию является *true*, означающее, что приложение будет перемещено в резервируемую очередь.

``yarn.scheduler.capacity.<queue-path>.show-reservations-as-queues`` -- необязательный параметр для отображения или скрытия очередей резервирования в пользовательском интерфейсе планировщика. Значение может быть только логическим (boolean), по умолчанию является *false*, то есть очереди резервирования скрываются.

``yarn.scheduler.capacity.<queue-path>.reservation-policy`` -- необязательный параметр: имя класса для использования в целях определения реализации *SharingPolicy* для проверки новых резервирований на предмет нарушения каких-либо инвариантов. Значением по умолчанию является *org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacityOverTimePolicy*.

``yarn.scheduler.capacity.<queue-path>.reservation-window`` -- необязательный параметр, представляющий время в миллисекундах, в течение которого *SharingPolicy* проверяет соблюдение ограничений в *Plan*. Значение по умолчанию составляет один день.

``yarn.scheduler.capacity.<queue-path>.instantaneous-max-capacity`` -- необязательный параметр: максимальная пропускная способность в процентах в виде числа с плавающей запятой (float), которую *SharingPolicy* позволяет зарезервировать одному пользователю. Значение по умолчанию равно *1*, то есть *100%*.

``yarn.scheduler.capacity.<queue-path>.average-capacity`` -- необязательный параметр: средняя допустимая пропускная способность, агрегируемая в *ReservationWindow* в процентах в виде числа с плавающей запятой (float), которую *SharingPolicy* позволяет зарезервировать одному пользователю. Значение по умолчанию равно *1*, то есть *100%*.

``yarn.scheduler.capacity.<queue-path>.reservation-planner`` -- необязательный параметр: имя класса для использования в целях определения реализации *Planner*, вызываемой при падении производительности *Plan* ниже зарезервированных пользователем ресурсов (из-за планового обслуживания или сбоев узла). Значением по умолчанию является *org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.SimpleCapacityReplanner*, сканирующее *Plan* и жадно удаляющее резервирования в обратном порядке (*LIFO*) до тех пор, пока зарезервированные ресурсы не оказываются в пределах пропускной способности *Plan*.

``yarn.scheduler.capacity.<queue-path>.reservation-enforcement-window`` -- необязательный параметр, представляющий время в миллисекундах, в течение которого *Planner* проверяет соблюдение ограничений в *Plan*. Значение по умолчанию составляет один час.


Динамическое автосоздание и управление leaf-очередями
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**CapacityScheduler** поддерживает автоматическое создание наследуемых leaf-очередей, настроенных с включенной данной функцией.

+ Настройка при помощи маппинга

**user-group queue mapping(s)**, перечисленные в ``yarn.scheduler.capacity.queue-mappings``, должны содержать дополнительный параметр очереди, в которую будет осуществляться автосоздание leaf-очередей. Свойства описаны выше в подразделе "Queue Mapping based on User or Group". Так же важно обратить внимание, что в таких родительских очередях необходимо включить автосоздание дочерних очередей, как указано далее.

Пример:

::

 <property>
    <name>yarn.scheduler.capacity.queue-mappings</name>
    <value>u:user1:queue1,g:group1:queue2,u:user2:%primary_group,u:%user:parent1.%user</value>
    <description>
      Here, u:%user:parent1.%user mapping allows any <user> other than user1,
      user2 to be mapped to its own user specific leaf queue which
      will be auto-created under <parent1>.
    </description>
  </property>

+ Конфигурация родительской очереди

Функция *Dynamic Queue Auto-Creation and Management* интегрирована с иерархией очереди **CapacityScheduler** и может быть настроена для *ParentQueue* для автоматического создания leaf-очередей. Такие родительские очереди не поддерживают возможность сосуществования автосозданных очередей вместе с другими предварительно сконфигурированными очередями. Свойства:

``yarn.scheduler.capacity.<queue-path>.auto-create-child-queue.enabled`` -- обязательный параметр: указывает для **CapacityScheduler**, что для заданной родительской очереди необходимо включить автосоздание leaf-очереди. Значение может быть только логическим (boolean), по умолчанию является *false*, то есть автосоздание leaf-очереди в *ParentQueue* не включено.

``yarn.scheduler.capacity.<queue-path>.auto-create-child-queue.management-policy`` -- необязательный параметр: имя класса для использования с целью определения реализации *AutoCreatedQueueManagementPolicy*, которая динамически управляет leaf-очередями и их производительностью в данной родительской очереди. Значением по умолчанию является *org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy*. Пользователи или группы могут отправлять приложения в автосозданные leaf-очереди в течение ограниченного времени и прекращать их использование. Следовательно, число leaf-очередей, автосозданных в родительской очереди, может быть больше, чем ее гарантированная пропускная способность. Текущая реализация политики позволяет либо настроить, либо обнулить производительность, исходя из доступности пропускной способности в родительской очереди и порядка отправки приложения через leaf-череди.

+ Настройка при помощи CapacityScheduler

Родительская очередь для автосоздания leaf-очередей поддерживает настройку параметров их шаблона. Автосозданные очереди поддерживают все параметры конфигурации leaf-очереди, за исключением *Queue ACL*, *Absolute Resource*. Списки ACL очереди в настоящее время не настраиваются в шаблоне, но наследуются от родительской очереди. Свойства:

``yarn.scheduler.capacity.<queue-path>.leaf-queue-template.capacity`` -- обязательный параметр: указывает минимальную гарантированную пропускную способность для автосоздаваемых leaf-очередей. В настоящее время конфигурации Absolute Resource не поддерживаются в автоматически созданных leaf-очередях.

``yarn.scheduler.capacity.<queue-path>.leaf-queue-template.<leaf-queue-property>`` -- необязательный параметр: для других параметров очереди, которые могут быть настроены в автосоздаваемых leaf-очередях, таких как *maximum-capacity*, *user-limit-factor*, *maximum-am-resource-percent* и прочие (`Свойства очереди`_).

Пример:

::

 <property>
    <name>yarn.scheduler.capacity.root.parent1.auto-create-child-queue.enabled</name>
    <value>true</value>
  </property>
  <property>
     <name>yarn.scheduler.capacity.root.parent1.leaf-queue-template.capacity</name>
     <value>5</value>
  </property>
  <property>
     <name>yarn.scheduler.capacity.root.parent1.leaf-queue-template.maximum-capacity</name>
     <value>100</value>
  </property>
  <property>
     <name>yarn.scheduler.capacity.root.parent1.leaf-queue-template.user-limit-factor</name>
     <value>3.0</value>
  </property>
  <property>
     <name>yarn.scheduler.capacity.root.parent1.leaf-queue-template.ordering-policy</name>
     <value>fair</value>
  </property>
  <property>
     <name>yarn.scheduler.capacity.root.parent1.GPU.capacity</name>
     <value>50</value>
  </property>
  <property>
      <name>yarn.scheduler.capacity.root.parent1.accessible-node-labels</name>
      <value>GPU,SSD</value>
    </property>
  <property>
      <name>yarn.scheduler.capacity.root.parent1.leaf-queue-template.accessible-node-labels</name>
      <value>GPU</value>
   </property>
  <property>
     <name>yarn.scheduler.capacity.root.parent1.leaf-queue-template.accessible-node-labels.GPU.capacity</name>
     <value>5</value>
  </property>


+ Управление конфигурацией Scheduling Edit Policy

Администраторы должны указать дополнительную политику редактирования  *org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.QueueManagementDynamicEditPolicy* со списком текущих политик в виде строки и разделенные запятыми в конфигурации ``yarn.resourcemanager.scheduler.monitor.policies``.

``yarn.resourcemanager.monitor.capacity.queue-management.monitoring-interval`` -- время между вызовами политики *QueueManagementDynamicEditPolicy* (в миллисекундах). Значение по умолчанию *1500*.


Другие свойства
~~~~~~~~~~~~~~~~~

+ Калькулятор ресурсов:

``yarn.scheduler.capacity.resource-calculator`` -- реализация **ResourceCalculator** для использования в целях сравнения ресурсов в планировщике. По умолчанию, *org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator*, используется только память, тогда как **DominantResourceCalculator** использует *Dominant-resource* для сравнения многомерных ресурсов, таких как память, процессор и пр. Значением должно быть имя класса Java ResourceCalculator.

+ Расположение данных

**Capacity Scheduler** использует **Delay Scheduling** для соблюдения ограничений месторасположения задач. Существует 3 уровня: node-local, rack-local и off-switch. Планировщик учитывает количество упущенных возможностей, когда локальность не может быть удовлетворена, и ждет, пока это число достигнет порогового значения, прежде чем ослабить ограничение положения до следующего уровня. Порог можно настроить в следующих свойствах:

``yarn.scheduler.capacity.node-locality-delay`` -- число упущенных возможностей, после которых **CapacityScheduler** пытается запланировать rack-local контейнеры. Как правило, значение должно быть установлено на количество узлов в кластере. По умолчанию устанавливается приблизительное количество узлов в одной стойке, которое составляет *40*. Должно быть положительное целое число.

``yarn.scheduler.capacity.rack-locality-additional-delay`` -- число дополнительных упущенных возможностей относительно node-locality-delay, после чего **CapacityScheduler** пытается запланировать off-switch контейнеры. По умолчанию значение равно *-1*, тогда в этом случае количество упущенных возможностей для назначения off-switch контейнеров рассчитывается по формуле ``L * C / N``, где *L* -- количество мест (узлов или стоек), указанных в запрос ресурса, *C* -- количество запрошенных контейнеров, а *N* -- размер кластера.

Важно обратить внимание, что эту функцию следует отключить, если *YARN* развертывается отдельно от файловой системы, поскольку локальность в таком случае не имеет смысла. Для этого необходимо установить ``yarn.scheduler.capacity.node-locality-delay`` в значение *-1*, тогда ограничение на местоположение запроса игнорируется.

+ Распределение контейнеров по NodeManager Heartbeat:

``yarn.scheduler.capacity.per-node-heartbeat.multiple-assignments-enabled`` -- допуск контейнеров нескольких назначений в одном heartbeat-сообщении NodeManager. По умолчанию устанавливается значение *true*.

``yarn.scheduler.capacity.per-node-heartbeat.maximum-container-assignments`` -- максимальное количество контейнеров, которое может быть назначено в одном heartbeat-сообщении NodeManager при заданном параметре multiple-assignments-enabled на *true*. Значение по умолчанию равно *100*, что ограничивает максимальное количество назначений контейнеров от 1 до 100. Установка значения в *-1* отключает ограничение.

``yarn.scheduler.capacity.per-node-heartbeat.maximum-offswitch-assignments`` -- максимальное количество off-switch контейнеров, которое может быть назначено в одном heartbeat-сообщении NodeManager при заданном параметре multiple-assignments-enabled на *true*. Значение по умолчанию равно *1*, что означает выделение только одного off-switch на heartbeat-сообщение.


Проверка конфигурации CapacityScheduler
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Конфигурацию **CapacityScheduler** можно проверить после завершения установки и настройки путем запуска кластера *YARN* через веб-интерфейс:

+ Запустить кластер YARN обычным способом;
+ Открыть веб-интерфейс ResourceManager;
+ Веб-страница */scheduler* должна показывать использование ресурсов отдельными очередями.



Изменение конфигурации очереди
------------------------------

Изменение конфигурации очереди через файл осуществляется путем редактирования *conf/capacity-scheduler.xml* и запуска ``yarn rmadmin -refreshQueues``:

::

 $ vi $HADOOP_CONF_DIR/capacity-scheduler.xml
 $ $HADOOP_YARN_HOME/bin/yarn rmadmin -refreshQueues

Удаление очереди через файл реализуется в два шага:

+ Остановка очереди: перед удалением leaf-очереди она не должна иметь запущенных/ожидающих приложений и должна быть в статусе STOPPED путем изменения ``yarn.scheduler.capacity.<queue-path>.state`` (`Администрирование и разрешения очереди`_). Перед удалением родительской очереди все ее дочерние очереди не должны иметь запущенных/ожидающих приложений и должны быть в статусе STOPPED. Родительская очередь также должна быть STOPPED;

+ Удаление очереди: удалить конфигурации очереди из файла и запустить обновление.

Изменение конфигурации очереди через API осуществляется путем использования резервного хранилища для конфигурации планировщика. Для этого могут быть настроены параметры в *yarn-site.xml*:

``yarn.scheduler.configuration.store.class`` -- тип используемого резервного хранилища;

``yarn.scheduler.configuration.mutation.acl-policy.class`` -- политика ACL может быть настроена для ограничения того, какие пользователи могут изменять какие очереди. Значением по умолчанию является *org.apache.hadoop.yarn.server.resourcemanager.scheduler.DefaultConfigurationMutationACLPolicy*, что позволяет только YARN-администраторам вносить изменения в конфигурацию. Другим значением является *org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.QueueAdminConfigurationMutationACLPolicy*, что позволяет вносить изменения очереди только в том случае, если вызывающий объект является администратором очереди;

``yarn.scheduler.configuration.store.max-logs`` -- изменения конфигурации регистрируются в бэк-хранилище, если используется leveldb или zookeeper. Эта конфигурация контролирует максимальное количество журналов аудита для хранения, удаляя старые журналы при превышении значения. По умолчанию *1000*;

``yarn.scheduler.configuration.leveldb-store.path`` -- путь к хранилищу конфигурации при использовании leveldb. Значением по умолчанию является *${hadoop.tmp.dir}/yarn/system/confstore*;

``yarn.scheduler.configuration.leveldb-store.compaction-interval-secs`` -- интервал сжатия конфигурации при использовании leveldb (в секундах). Значение по умолчанию *86400* (один день);

``yarn.scheduler.configuration.zk-store.parent-path`` -- путь к root-узлу zookeeper для хранения связанной с конфигурацией информации при использовании zookeeper. Значением по умолчанию является */confstore*.

При включении конфигурации планировщика через ``yarn.scheduler.configuration.store.class``, отключается *yarn rmadmin -refreshQueues*, то есть исключается возможность обновления конфигурации через файл.

.. important:: Функция изменения конфигурации очереди через API находится в альфа-фазе и может быть изменена



Обновление контейнера
-----------------------

*Экспериментально. API может измениться в будущем*

Как только **Application Master** получает контейнер от **Resource Manager**, Master может запросить у Manager обновить некоторые атрибуты контейнера. В настоящее время поддерживаются только два типа обновлений контейнера:

+ Resource Update: когда Application Master может запросить Resource Manager обновить ресурсный размер контейнера. Например: изменить контейнер *2GB, 2 vcore* на контейнер *4GB, 2 vcore*.

+ ExecutionType Update: когда Application Master может запросить Resource Manager обновить ExecutionType контейнера. Например: изменить тип выполнения с *GUARANTEED* на *OPPORTUNISTIC* или наоборот.

Этому способствует **Application Master**, заполняющий поле *updated_containers*, представляющее собой список типа *UpdateContainerRequestProto* в *AllocateRequestProto*. Master может сделать несколько запросов на обновление контейнера в одном вызове.

Схема *UpdateContainerRequestProto* выглядит следующим образом:

::

 message UpdateContainerRequestProto {
   required int32 container_version = 1;
   required ContainerIdProto container_id = 2;
   required ContainerUpdateTypeProto update_type = 3;
   optional ResourceProto capability = 4;
   optional ExecutionTypeProto execution_type = 5;
 }

*ContainerUpdateTypeProto* является перечислением:

::

 enum ContainerUpdateTypeProto {
   INCREASE_RESOURCE = 0;
   DECREASE_RESOURCE = 1;
   PROMOTE_EXECUTION_TYPE = 2;
   DEMOTE_EXECUTION_TYPE = 3;
 }


В соответствии с приведенным перечислением планировщик в настоящее время поддерживает изменение типа обновлений контейнера Resource Update либо ExecutionType Update в одном запросе.

**Application Master** также должен предоставить последнюю версию *ContainerProto*, полученную от **Resource Manager** -- это контейнер, который Manager запрашивает на обновление.

Если **Resource Manager** может обновить запрошенный контейнер, то тогда обновленный контейнер возвращается в поле списка *updated_containers* типа *UpdatedContainerProto* в возвращаемом значении *AllocateResponseProto* того же самого вызова или одного из последующих.

Схема *UpdatedContainerProto* выглядит следующим образом:

::

 message UpdatedContainerProto {
   required ContainerUpdateTypeProto update_type = 1;
   required ContainerProto container = 2;
 }


Здесь указывается тип выполненного обновления для контейнера и объект обновленного контейнера, содержащий обновленный токен.

Затем токен контейнера может использоваться **Application Master** для запроса соответствующего **NodeManager** либо запуска контейнера, если он еще не запущен, либо для обновления контейнера с использованием обновленного токена.

Обновления контейнера *DECREASE_RESOURCE* и *DEMOTE_EXECUTION_TYPE* выполняются автоматически -- **Application Master** не должен явно запрашивать **NodeManager**, чтобы уменьшить ресурсы контейнера. Другие типы обновлений требуют, чтобы Master явно запрашивал об обновлении.

Если для параметра конфигурации ``yarn.resourcemanager.auto-update.containers`` задано значение *true* (по умолчанию *false*), **Resource Manager** обеспечивает автоматическое обновление всех контейнеров.
